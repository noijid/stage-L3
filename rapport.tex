\documentclass[a4paper]{article}

\usepackage[latin1]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage{layout}
\usepackage{dsfont}
\usepackage[square,numbers,sort&compress]{natbib}
\usepackage[francais]{babel}
\usepackage[top=2cm, bottom=3cm, left=2cm, right=2cm]{geometry}
\usepackage{listings}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{algorithmique}

\title{Rapport de stage : Factorisation et calcul de logarithmes discrets : les algorithmes de cribles}
\author{Oijid Nacim} 
\date{Juin-Juillet 2018}

%Macros
\newtheorem{definition}{Définition}
\newtheorem{nota}{Notation}
\newcommand{\p}{\mathbb{P}} 
\newcommand{\z}{\mathbb{Z}} 
\newcommand{\ztz}{$\mathbb{Z}/2\mathbb{Z}$} 
\newcommand{\al}{\alpha} 
\newcommand{\ere}{\textsuperscript{ère} }
\newcommand{\er}{\textsuperscript{er} }
\newcommand{\eme}{\textsuperscript{ème} }



\begin{document}

\maketitle

\begin{abstract}

\end{abstract}

\section{Context du stage} 

\subsection{L'équipe}


Du 3 Juin 2018 au 13 Juillet 2018, j'ai effectué un stage au sein du LIP (situé à Lyon), dans l'équipe Aric travaillant en cryptographie. Au cours de ce stage, j'ai pu m'intéresser à la factorisation de grands entiers, ce qui a une place très importante en cryptographie.
Plus largement, ce stage a été l'opportunité pour moi d'être initié à la recherche.

\subsection{Connaissances Annexes}
Au-delà d'enrichir mes connaissances en informatique théorique et de me forger une culture général, ce stage m'a permis de développer mes compétences sous différents aspects : en programmation, en me faisant notamment manipuler un langage qui m'étais encore inconnu et très utile dans le secteur de la recherche en informatique et mathématiques : Sage, qui est une surcouche de Python; et en m'efforçant d'utiliser Bash dans un terminal, git et latex, afin de mieux développer mes capacités avec ces outils informatiques très utiles dans le monde de la recherche. De plus, j'ai appris à manipuler ssh, afin de pouvoir exécuter des calculs depuis chez moi sur les machines du LIP.

\subsection{Le monde de la recherche}

J'ai également profité de ce stage pour avoir une meilleure culture générale sur différents domaines de recherche. En assistant à des "crypto-meeting", à des exposés sur des sujets divers comme Shortest Vector Problem (SVP), à  une soutenances de stage de M2 sur les Simple Stochastic Games (SSG), ou en lisant des articles sur différents sujets de recherche, comme RSA, l'algorithme de Shor (en informatique quantique). 
 
 \subsection{Le sujet}

Mon stage au sein de cette équipe a consisté essentiellement à l'étude, la compréhension et l'écriture du code d'algorithme de factorisation de grands entiers : Quadratic Sieve (crible quadratique) et Number Field Sieve (crible algébrique). Puis d'essayer une potentielle amélioration du second.
Ces deux algorithmes ont pour but de factoriser un entier en cherchant des congruences de carrés. Ces algorithmes sont très efficaces puisqu'ils ont permis de factoriser de nombreux nombres RSA (les nombres RSA-$n$ sont des nombres de $n$ bits produite de deux "grands" nombres premiers)
Ils ont un intérêt certains puisque QS a notamment permis de factoriser RSA-129, le 2 Avril 1994. Puis, le 10 Avril 1996, NFS a factorisé RSA-130, et a ainsi battu le record établis par QS 2 ans auparavant. Puis, en 1999, SNFS (une version modifiée de NFS) a factorisé un nombre à 211 chiffres (701 bits) : $(10^{211}-1)/{9}$. (selon \cite{ref6} ). Dès lors, NFS a obtenu de nombreux records de factorisation comme RSA-576 en 2003, RSA-640 en 2005 et RSA-768 en 2009.

\subsection{Ma participation à la recherche}

Ma modeste participation au monde de la recherche consistait à modifier le code du crible algébrique afin d'essayer de l'accélérer. Ainsi, en comparant des temps de calculs, d'une même base d'implémentation, mais en utilisant d'une part l'algorithme habituel, et d'autre part celui qui fût l'objet principale de mon stage, ce stage a permis d'observer l'intérêt du dit algorithme, et si celui ci permettais de gagner en temps de calcul. 



\section{Crible quadratique(Quadratic Sieve)}

\subsection{Présentation}

\subsubsection{Définitions et Notations}

\begin{nota}[Nombre premiers]
$ \p $ \hspace{0.5 cm}Désignera l'ensemble des nombre premiers.
\end{nota}

\begin{definition}[Nombre friable]
un entier est dit B-friable (resp friable dans $P \subset$ $ \p $) si l'ensemble de ses diviseurs premiers sont inférieurs à B (resp dans P)
\end{definition}

\begin{definition}[Complexité L]
On notera $L_N(x,y)$ avec $0 \le x \le 1$ la complexité tel que
\begin{equation}
L_N(x,y) = \lfloor \exp((y + o(1))\log(N)^x \log(\log(N))^{1-x} ) \rfloor
\end{equation}
\end{definition}

\begin{nota}
On notera simplemant $L(x)$ pour $L_N(x,\cdot)$ avec $N$ implicitement déterminé par l'entier à factoriser (car la dépendance en $x$ influe beaucoup plus que celle en $y$).
\end{nota}

\subsubsection{L'algorithme}
Le premier algorithme écrit durant le stage est celui du Quadratic Sieve.
Cet algorithme, s'inspire du crible d'Eratosthène pour factoriser de grands entiers.

étant donnée un entier $N=a*b$ avec $a\ne1$ et $b\ne1$ (on suppose $N$ non premier). On cherche à factoriser $N$ en l'écrivant $N =u^2 - v^2 = (u-v)*(u+v)$.
Ceci est toujours possible car $a*b = (\frac{a+b}{2})^2 - (\frac{a-b}{2})^2$. On constate alors, qu'étant donnée $u$, il faut que $u^2-N$ soit un carrée pour avoir une telle décomposition.


Pour se faire, on distingue 2 types de diviseurs : les petits, qui seront recherchés de manière exhaustive (jusqu'à $\log(N)$); et les plus gros, qui seront recherchés avec l'algorithme décrit plus loin.


On modifie alors le crible d'Eratosthène pour créer une fonction $Eratosthene (x,I,J,P)$ décrite dans \cite{ref3} qui prends en arguments 3 entiers $x$, $I$ et $J$ et une liste de nombre premier $P$ et qui renvoie les nombres $y$ compris entre $I$ et $J$, et tel que $y^2 - x$ soit friable dans $P$, ainsi que leurs décomposition, et la valeur de $y^2-x$. (En pratique, on peut autoriser un des facteurs en dehors de $P$, et c'est d'ailleurs comme ça que je l'ai implémenté pour gagner en temps de calcul, mais ce n'est pas obligatoire).

Comment choisir $P$ une bonne base de facteur ? 
Il en faut ni trop peu (sinon, il n'y a pas assez de nombre qui se décompose sur cette base) ni trop (sinon, il faut trop de vecteurs pour obtenir une relation de dépendance linéaire). 
Une étude mathématique évoquée dans \cite{ref1} nous indique qu'il suffit d'observer les nombres premiers plus petit qu'une certaine fonction 
$L(1/2) $ (fonction que l'on retrouve souvent en complexité).
Dès lors, on ne s'intéresse qu'aux entiers $p$ tel que $N$ soit un carrée modulo $p$. En effet, si $N$ n'est pas un carrée modulo p, $u^2-N$ n'est jamais divisible par $p$. Donc p ne peut pas intervenir dans la décomposition de v. Donc peut être retiré de $P$.


Dès lors, si l'on note $B = |P|$,  et qu'on s'intéresse uniquement à la valuations p-adique dans \ztz ; et si l'on trouve au moins $B+1$ valeurs $u_i$ telle que $v_i=y^2-x$ soit friable dans $P$(à l'aide du crible d'Eratosthène modifié), ces vecteurs seront liés, et donc, on disposera d'une relation de dépendance linéaire $(\nu_1, ... , \nu_{B+1})$ de ces valuations dans \ztz, ce qui signifie que leur produit sera un carré.

On aura alors : $u^2 = \Pi_{i=1}^{B+1} (u_i^2)^{\nu_i} \equiv v^2 = \Pi_{i=1}^{B+1} (v_i^2)^{\nu_i} [n]$ .
Il reste alors à vérifier que $\mathop{pgcd}(u-v,N) \notin \{1,n\}$ (d'après un théorème qui est vérifié en pratique mais dont je ne me suis pas plongé dans la preuve, cela est vrai avec une probabilité supérieure à $1/2$ ) 



\subsection{Résultats}

L'implémentation du crible algébrique a occupé ma 1\ere semaine de stage(puis j'ai effectuée quelques modifications de temps à autres la seconde).
Ma première implémentation de l'algorithme permettait de factoriser des nombres ayant jusqu'à 30 chiffres en 15 à 20 minutes.
Ma seconde implémentation permets de factoriser des nombres ayant jusqu'à 40 chiffres ( 132 bits) en environ 10 minutes. La seconde implémentation prends également en compte les large prime variations. C'est à dire, qu'au lieu de ne considérer que des entiers friables, on considérait des entiers n'ayant qu'un facteur premier n'étant pas dans la base de crible. (donc inférieur au carré du plus grand nombre premier de la base, pour ne pas avoir à vérifier la primalité de ce nombre).
Lorsque l'on lance l'exécution de mon implémentation sur des entrées plus longue, l'exécution s'arrête car un trop grand espace mémoire est utilisé (cela est du au vector des facteurs des nombre).


\section{Crible algébrique (Number Field Sieve)}

\subsection{Introduction}

\subsubsection{Lien entre le crible quadratique et le crible algébrique}

L'idée principale du crible algébrique est que la recherche de carrés différents dans $\z$ et dans $\z/N\z$ permets d'obtenir une relation de la forme $u^2 \equiv v^2 [N]$. En effet, on considère des couples $(x_i,y_i)$ tel que $x_i^2 \equiv y_i [N]$ puis on cherche une combinaison des $y_i$ de manière à obtenir un carrée.
Ainsi, de deux factorisations, une dans $\z$ et une dans $\z/N\z$ on obtient une factorisation de $N$

Le crible algébrique pousse cette idée plus loin, en considérant, des polynômes de degré supérieurs sortes de réduire les tailles des éléments à factoriser. Ainsi, au lieu de factoriser dans $\z/N\z$ et dans $\z$, on factorise dans $\z[\al_1]$ et $\z[\al_2]$ avec $\al_1$ et $\al_2$ des racines des polynômes à considérer. de sortes à factoriser des nombres plus petits. pour aboutir à la même congruence.

\subsubsection{Idée de l'algorithme}
Une première version de NFS est présentée dans \cite{ref2} permettant de factoriser des entiers s'écrivant $n = x^3 +k$ avec $k$ petit. En pratique, NFS calcul une factorisation d'un entier $N$ en trouvant deux polynômes $f$ et $g$ ayant une racine commune $m$ modulo $n$. Puis, l'algorithme se place dans deux anneaux $\z[\al_1]$ et $\z[\al_2]$ avec $\al_1, \al_2$ racines respectivement de $f$ et $g$ où, comme pour le crible quadratique, le but est de trouver $x$ et $y$ tel que $x^2 \equiv y^2 [n]$, mais les nombres en question sont ici plus petits. Puis à l'aide de morphismes, en envoyant $\al_1$ et $\al_2$ sur $m$, on se ramène à $\z/n\z$ où les congruences reste valable.
Cet algorithme peut aussi être utilisé pour calculer des logarithmes discrets selon \cite{ref7} mais ce n'est pas la partie traitée ici

\begin{figure}
\centering
\includegraphics[scale=0.4]{NFS1.png}
\caption{ Idée générale de NFS }
 \end{figure}

\subsubsection{Pourquoi utiliser un tel algorithme}

En pratique, l'algorithme de crible quadratique a permis de factoriser de nombreux grands entiers, mais la complexité en $L(1/2)$ l'empêche d'être suffisamment compétitif sur des trop grosses entiers. L'algorithme du crible algébrique quand à lui est plus rapide sur les gros entiers, avec une complexité en $L(1/3)$ (il est plus rapide que le crible quadratique à partir de 90 chiffres) et est le meilleur algorithme de l'informatique classique pour factoriser n'importe quel entier. (de meilleurs algorithmes existent pour des formes d'entier particuliers, et l'algorithme de Shor a une meilleure complexité théorique en informatique quantique).


\subsection{Présentation}

\subsubsection{Definitions}

\begin{definition}[Unité]
Dans $\z[\al]$, muni de la norme multiplicative $N$, on appelle unité tout nombre $U$ tel que $N(U) = 1$.
\end{definition}

\begin{definition}[Nombre premier]
Dans $\z[\al]$, on appelle nombre premier, tout nombre $p$ vérifiant $p = ab => N(a)=1$ ou $N(b)=1$
\end{definition}

\subsubsection{Choix des polynômes}
Dans le cas général, on cherche deux polynômes, $f$ et $g$, irréductibles et premiers entre eux dans $\mathbb{Q}[X]$, ayant des coefficients pas trop grands, afin de ne pas avoir des normes trop grandes pour les $a - \al b$ dans $\z[\al_1]$ et $\z[\al_2]$. En effet, plus les normes des nombres seront petites, plus il sera probable qu'ils soient friables, et donc cela permets d'accélérer la recherche de congruences. On choisit donc, en général, un certain entier  $m$, puis on prends pour les coefficients de $f$, ceux de l'écriture de $n$ en base $m$, et on prends $g(x) = x - m$ de sortes que $f$ et $g$ et m comme racine commune modulo $n$. On ne veut pas non plus $f$ de degré trop grand, donc on ne prends pas m trop petit. Selon \cite{ref6}, on fixe $d = 4$ ou $5$ (plus généralement, $d = \log(N)^{1/3}$ est la valeur optimale de l'analyse théorique de la complexité) le degré de $f$ et on prends $m$ proche de $n^{\frac{1}{d+1}}$.

\subsubsection{Recherche de congruence}
On suppose dans cette partie, les polynômes $f$ et $g$ fixés et on note $m$ leur racine commune. On se munit de deux normes multiplicatives $N_1$ et $N_2$ sur les anneaux $\z[\al_1]$ et $\z[\al_2]$.  On se fixe alors deux bases de nombres premiers $FB_1$ et $FB_2$ dans $\z[\al_1]$ et $\z[\al_2]$, et on cherche des paires de nombres premiers entre eux $a_i$ et $b_i$ tel que $a_i - \al_1b_i$ et $a_i - \al_2b_i$ soient friables. En raison du grand nombre de divisions à tester, on impose d'abord un spécial-Q, c'est à dire, qu'on recherche seulement les paires $(a_i,b_i)$ tel que $a_i-\al b_i$ soit divisible par Q Pour se faire, on divise la base en deux sous partie : "la base de crible" noté $ffb$ (free factor base), et la "grande variation en nombre premier" noté $B$. On impose alors d'être divisible par un spécial-Q entre $ffb$ et $B$, puis que les $(a_i - \al b_i)/Q$ soit fbb-friable.
Cela nous fait perdre certaines relation, mais accélère énormément le calcul.
Cela permets de ne chercher à factoriser que des entiers déjà un peu plus petit (de la taille plus petit de $\log(Q)$ ). On peut alors cribler de façon linéaire : en parcourant toutes les valeurs de $b$ pour chaque valeur de $a$ et donc, on peut effectuer parallèlement les calculs à $a$ fixé. On gagne ainsi un peu de temps de calcul . En pratique, cette étape s'effectue en calcul distribué mais en utilisant des algorithmes de cribles plus efficaces que celui linéaire, ce qui sera développé dans la sous-partie suivante : l'algorithme de Franke-Kleinjung.
En pratique, on recherche au moins $ \#(FB_1) + \#(FB_2) + 1 $ relations pour être sur d'avoir une décomposition en carré.

\subsubsection{Le crible par maille}

Le crible linéaire, bien qu'intuitif (car proche de celui d'Eratosthène), a le défaut de devoir explorer chaque valeur de b, et est donc efficace seulement sur les réseaux denses. En effet, pour certains idéaux (quand $p$, le facteur recherché est proche de $I$ la longueur de l'intervalle de recherche sur $a$), le réseau est beaucoup moins dense, et il est possible que seulement quelques entiers soient divisible par $p$ (un nombre beaucoup plus petit que b). L'algorithme de Franke-Kleinjung propose alors une solution, en trouvant une base de 2 vecteurs, $u$ et $v$ tel que, pour passer d'un point au suivant (en triant les points par ordonnée), il suffit d'ajouter $u$, $v$, ou $u+v$ au premier. (cet algorithme permettant de trouver $u$ et $v$ fait appel à des résultats d'algèbre de $(\z)^n$ et de calcul de pgcd mais n'a pas été étudié au cours de ce stage). 
Dès lors, le crible devient beaucoup  plus efficace, puisqu'il suffit de trouver un point (en général 0), puis la complexité pour trouver tous les autres devient linéaire en le nombre de point à trouver et non en la taille de l'intervalle de recherche. On en déduit donc que, si le réseau est dense, le crible linéaire est aussi efficace que le crible par maille, mais sans nécessité de précalcul (des vecteurs $u$ et $v$ ci-dessus).

Résultat observé. J'ai lancé la 1\ere version de small NFS fourni par Laurent, en utilisant soit seulement le crible linéaire, soit le crible linéaire sur les petits nombre premiers, et le crible par maille sur les gros nombre premiers (ceux dépassant la taille de l'intervalle de recherche). On ne gagne presque pas de temps. une analyse du code, et des résultats retournés montre que cela est dû à un réseau trop dense même pour les plus grosses valeurs des nombres premiers $p$ considérés. De plus, une analyse plus poussé nous montre que lors de la recherche de relations, les étapes les plus longues étaient les temps d'initialisation des normes et de calculs des cofacteurs (car l'algorithme fournit par Laurent observé si les nombres étaient friables sans retenir les facteurs trouvés).

\subsubsection{Algèbre linéaire}
On utilise alors un la même procédure que dans le crible quadratique, en utilisant les formes factorisées des nombre précédents pour obtenir une congruence de carrés.  Pour ce faire, on décompose les valeurs de $a_i-\al b_i$ en facteur premier dans $\z[\al_1]$ et $\z[\al_2]$. On s'intéresse aux exposants modulo $2$ des facteurs premiers apparaissant dans les décompositions. Et, à l'aide d'outils offerts par Sage, on calcul le noyau de la matrice composé de ces exposants pour trouvé une combinaison de $a_i- \al b_i$qui est à la fois un carré pour $i = 1$ et pour $i = 2$.

\subsubsection{Factorisation}
Une fois qu'on a obtenu le noyau de la matrice, on injecte $\z[\al_1]$ et $\z[\al_2]$, dans $\z/N\z$ avec $\al_i -> m$. La congruence est alors conservée car m est racine commune de $f$ et $g$. L'algèbre linéaire nous dit alors que les 2 factorisations obtenus dans les 2 anneaux respectifs sont bien congru l'un à l'autre modulo N, et on obtient alors une relation de la forme $u^2 \equiv v^2 [N]$, ce qui donne une factorisation de N avec une probabilité supérieure à 1/2 (encore une fois, ce résultat n'a pas été étudié).

\subsubsection{Utilisation habituelle de NFS}
En règle général, le crible algébrique est utilisé avec l'un des deux polynômes (disons $g$) de degré 1. En particulier, on prend souvent $g(x) = x- m$. Dès lors, on qualifie d'algébrique la partie de l'algorithme portant sur $f$ et de rationnelle celle portant sur $g$ (car $\z[\al_2] = \z$). Le spécial-Q est alors imposé que dans la partie algébrique de NFS (car c'est la partie la plus dur à factoriser).


\section{ NFS multi-special-Qs: objet principal de mon stage}

\abstract
L'objectif principal de mon stage était de partir de l'implémentation de Laurent de NFS :  Small NFS (idée qu'il a développé dans sa thèse), et d'essayer au lieu d'imposer un spécial-Q d'un seul côté, d'en imposer un de chaque côté.
L'avantage, et que cela équilibre les complexités de recherche de relation entre les différents côtés de l'équation. Le désavantage, est que le fait d'imposer 2 spécial-Q, et donc 2 diviseurs, diminue le nombre de relation que l'on va trouver.
Je devais donc analyser les différents temps de calculs de Small NFS avec 1 ou 2 spécial-Q pour observer si, en pratique, on y gagne ou pas à imposer 2 spécial-Q.
Ainsi, la seule partie qui change est la recherche de relation.
\subsection {Algorithme}

\begin{nota}[Entrée des algorithmes]

$N$ : nombre a factoriser;

$f$ : polynome; 

$B$ : Limite pour tester la friabilité des nombres (grande limite);

$qrange$ : liste de 2 éléments : les bords de recherche de spécial-Qs;

$H$ : limite de la base sur laquelle on factorise 

thresh : liste à 2 éléments : valeurs à partir de laquelle, on considère qu'un entier est trop gros pour être smooth (après crible);

construit\_ideal($f_1$,$q_1$) :  fonction écrite par Laurent, renvoie la liste des idéaux de normes $q_1$ dans $\z[X]/f_1\z[x]$;

ideal\_matrix(ideal) : renvoie une base du réseau créé par l'idéal;

\end{nota}

\begin{pseudocode}[shadowbox]{NFS avec 2 spécial-Qs}{N, f_1, f_2, B_1, B_2, qrange_1, qrange_2, H_1, H_2, thresh}

R = [$ $]
\POUR q_1$ premier dans $qrange_1 \FAIRE
\BEGIN
   ideal_1 = construit\_ideal(f_1, q_1) \\
   \POUR q_2$ premier dans $qrange_2   \FAIRE
   \BEGIN
      ideal_2 = construit\_ideal(f_2, q_2) \\
      \POUR i \in ideal_1 \FAIRE
      \BEGIN
         \POUR j \in ideal_2 \FAIRE
         \BEGIN
            Q = crible\_special\_Q(i, j, f_1, f_2, B_1, B_2, fbb_1, fbb_2, H_1, H_2, thresh)\\
            R = R \cup Q
         \END\\
      \END\\
   \END\\
\END\\
\RETURN{R}
\end{pseudocode}\\

\begin{pseudocode}[shadowbox]{crible\_special\_Q}{ideal_1, ideal_2, f_1, f_2, B_1, B_2, fbb_1, fbb_2, H_1, H_2, thresh}

M = $matrice de l'intersection des idéaux$ \\
R = [$ $] \\
\COMMENT{Initialisation des Normes}\\
array\_norms = [$ $] \\
\POUR i_0, \in [ - H_0,H_0] \FAIRE
\BEGIN
  empile(array\_norms, [ [ $ $] \POUR i \in [ 0, H[1] ] ] ) \\
   \POUR i_1 \in [0, H_1] \FAIRE
   \BEGIN
      (a_0,a_1) = (i_0, i_1)*M \\
      a = a_0 + a_1x \\
      array\_norms[i_0 + H[0] ] [i_1] = [N(a, f_1) / ideal_1[0], N(a, f_2) / ideal_2[0]] \\
      \COMMENT{On retire aux normes la participation des idéaux qu'on crible}
   \END
\END\\

\COMMENT{Crible}\\
\COMMENT{ les crible mesurent la friabilite en modifiant les norms} \\
\POUR j \in \{1,2\} \FAIRE
\BEGIN
   \POUR  i$ premier, inférieur à $fbb_j   \FAIRE
   \BEGIN
   r = racine(M,i) \\
   $Utilise le crible pour réduire les normes $
       \END
\END\\

\COMMENT{Cofactorisation}\\
\POUR y =a _0 + a_1x $ dans le réseau avec $a_0, a_1$ premiers entre eux $ \FAIRE
   \BEGIN
      \SI $ la norme restant de $y$ est inférieur à Thresh $ \FAIRE
         \BEGIN
               empile(R, factoriser(a_0 + a_1*x))
   \END
\END\\
\RETURN{R}


\end{pseudocode}\\

\subsection{CADO-NFS : un outil utile}

Pour tester de manières plus efficaces mes algorithmes, j'utilisais Cado-NFS que l'on trouve ici : \cite{ref8}. 
C'est une implémentation de NFS permettant d'obtenir des résultats intermédiaire, d'exécuter plusieurs parties de l'algorithmes seuls. Ainsi, j'utilisais cette implémentation pour me placer dans un cas favorable à l'utilisation de 2 spécial-Qs, et car l'objet de mon stage était portée seulement sur la recherche des relations, et non sur l'écriture de tout le code de NFS.

CADO-NFS permets, en lui donnant en entrée un nombre $N$ que l'on souhaite factoriser, d'obtenir des polynômes ayant une racine commune modulo $N$. Puis, en utilisant ces polynômes en entrée du programmes que j'ai écris, on obtient des relations que l'on peut à nouveau donner à CADO-NFS pour qu'il termine la factorisation.

\subsection{Résultats observés}

\begin{abstract}
Les résultats de cette parties sont, si ce n'est pas précisés, obtenus sur la factorisation d'un $C_{30}$ (nombre composé de 30 chiffres digitales, donc 99 bits).
L'implémentation de NFS utilisée est SMALL-NFS (écrite par Laurent) à laquelle j'ai ajouté le crible par maille.
\end{abstract}

\subsubsection{Conséquences sur les doublons}

Le principal désavantage d'imposer 2 spécial-Qs, est l'augmentation du nombre de doublons. En effet, les doublons générés par un seul special-Q sont maintenus quand on en mets. Mais, de plus, le fait d'observer 2 spécial-Qs peut créer des doublons. On traite le cas ou un des 2 côté est rationnel, les autres cas étant plus compliqué mais laissant le même phénomène se produire. Si $p_1,p_2 \in $ $ \p $ tel qu'il existe des spécials Q à la fois de norme $p_1$ et $p_2$ des 2 côtés, considérons 4 spécial-Qs $(p_1, r_{1, 1})$ et $(p_2, r_{1, 2})$  gauche et $(p_1, r{2, 1})$ et $(p_2, r_{2, 2})$ à droite. La probabilité que le réseau formé par $(p_1, r_{1, 1})$ et $(p_2, r_{2, 2})$ soit le même que celui formé par  $(p_2, r_{1, 2})$ et $(p_1, r{2, 1})$ est alors plus grande que la probabilité que $r_{1, 1}=r_{2, 1}$ et $r_{1,2} = r{2,2}$. En admettant que les racines des polynomes considérés soient équirépartis, cette probabilité est alors plus grande que $ \frac{1}{p_1p_2}$. On crée donc en moyenne $ \Sigma \frac{1}{p_ip_j} $ qui est une somme divergente (équivalente à $\ln(\ln(B_1)) \ln\ln(B_2)$ 


\subsection{Problème observé}

L'utilisation d'un fonction auxiliaire "two-quadratics" de CADO-NFS \cite{ref8} donne 2 polynômes de degré 2 ayant une racine commune modulo $N$.
La complexité théorique de l'algorithme dit que, en théorie, peu importe la qualité des polynômes, le nombre de relation trouvés varie peu, et la complexité reste la même.
Une première utilisation de two-quadratics a cependant donné des polynomes ayant des coefficients non équilibrés $x^2 + x - 243 192 642 611 455$ et $x^2 - 18x + 2 679 089 722 710 863$ qui ont retournés de très mauvais résultats : moins de 10\% du nombre de relation à trouver pour avoir une chance de factoriser le nombre.
Ainsi, cet échec a permis de montrer que les complexités théoriques ne sont pas nécessairement vrais en pratique.

\subsubsection{Valeurs numériques}

Une fois le problème des polynômes constaté, grâce à un test sur CADO-NFS qui ne trouvait pas non plus assez de relations, j'ai relancé l'algorithme avec d'autres polynômes, encore générés par "two-quadratics" mais avec des coefficients bien plus équilibrés :  $1 128 515 x^2  - 829 877 x - 34 442 033$ et $-84 478 329 x^2 + 62 122 898x - 11 233 320$
L'exécution de small-NFS donne 353 829 relations en observant 3297 spécial-Qs, pour une base de facteur pour la factorisation allant jusqu'à $2^8$ et une base de crible pour les spécial-Qs allant jusqu'à $2^15$. Il fallait donc au moins 13 092 relations Le temps de crible était de une heure huit minutes. L'étape la plus longue est toujours l'initialisation des normes qui dure 5 heures, mais qui a été écrite par Laurent, et que je ne peux pas modifier.

Concernant l'utilisation de deux spécial-Qs, lancer l'algorithme sur les mêmes paramètres renvoie moins de relation, en un temps de calcul plus long. ( seulement 82 812 relation pour 30 heures de calculs) en raison du grand nombre de doublons. Ainsi, il semblerait à première vue que l'utilisation de 2 spécial-Qs ne présente pas d'avantage. Cependant, l'avantage d'observer deux spécial-Qs est que l'on observe d'avantage de réseaux. (pour les mêmes paramètres, si l'on observe $n$ réseaux pour un seul sépcial-Q, on en observe dans l'ordre de $n^2$ pour deux spécial-Qs Donc, si l'on compare l'utilisation de 2 spécial-Qs sur des paramètres plus petits avec l'utilisation de small-NFS, on observe :

\section*{Conclusion}

\bibliographystyle{alpha}    
\bibliography{bibliographie}

\end{document}